{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yonado/My-Portfolio/blob/master/Koboldcpp_colab_unofficial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is unofficial colab that use Hibikiass model list. because official one only support huggingface model link which most ppl found it harder to use\n",
        "# Visual guide in case you are confuse\n",
        "---\n",
        "Slide: https://docs.google.com/presentation/d/13a-4LKpDHYKXMyD3cWF1Xj2zkImD203M0g5SPiAfWNQ/edit#slide=id.g306911e8e78_134_0\n",
        "\n",
        "Video PC: https://www.youtube.com/watch?v=WQfrGUrgraM\n",
        "\n",
        "Video Phone: https://www.youtube.com/watch?v=Zyvz96PD9PE"
      ],
      "metadata": {
        "id": "CJpUgQa_qDPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Default Model Review\n",
        "\n",
        "\n",
        "\n",
        "| Model | Instruct | Review |Context Size |\n",
        "| --- | --- | --- | --- |\n",
        "| [Kunoichi(7b)](https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix)|Alpaca/Vicuna|New meta for 7b|**8k base / 24k max**|\n",
        "| [WizardIceLemonTeaRP(7b)](https://huggingface.co/mradermacher/WizardIceLemonTeaRP-32k-GGUF)|Alpaca/Vicuna|Has mild censorship, so it gives good result for noncon situation|**8k base / 24k max**|\n",
        "| [WizardLaker(7b)](https://huggingface.co/mradermacher/WizardLaker-7B-GGUF)|Alpaca/Vicuna|Has mild censorship, so it gives good result for noncon situation|**8k base / 24k max**|\n",
        "| [StunnaMaid(7b)](https://huggingface.co/Lewdiculous/Nyanade_Stunna-Maid-7B-v0.2-GGUF-IQ-Imatrix)|Alpaca/Vicuna|Got lot of praise for prose|**8k base / 24k max**|\n",
        "| [LemonKunoichiWizard(7b)](https://huggingface.co/mradermacher/LemonKunoichiWizardV3-GGUF)|Alpaca/Vicuna|My favorite for 7b if using on janitor. good for newbie|**8k base / 24k max**|\n",
        "| [Llama-3-Halu-Blackroot(8k)](https://huggingface.co/mradermacher/Halu-8B-Llama3-Blackroot-GGUF)|Llama3|Gives long response because Blackroot is storytelling lora, Halu part gives the model unhinged brain|**8k base/ 32k max**|\n",
        "| [Llama-3-Lumimaid(8b)](https://huggingface.co/Lewdiculous/Llama-3-Lumimaid-8B-v0.1-OAS-GGUF-IQ-Imatrix)|Llama3|From creator of noromaid. offer you a model that has balance sfw/nsfw bias|**8k base/ 32k max**|\n",
        "| [Llama-3-Daybreak-Lumimaid(8b)](https://huggingface.co/mradermacher/llama3-daybreak-lumimaid0.1-8b-hf-GGUF)|Llama3|Daybreak merge to makes lumimaid to be better at story writing, gives longer response|**8k base/ 32k max**|\n",
        "| [Llama-3-Stheno(8b)](https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.3-32K-GGUF-IQ-Imatrix)|Llama3|From chef who cooked fimbulvetr, now making claude opus clone in llama3. have nsfw bias|**8k base/ 32k max**|\n",
        "| [Llama-3-Stheno-ULTRA(8b)](https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF)|Llama3 / Command R|Stheno that was upscaled to be double precision. somehow using command r instruct makes it more creative|**8k base/ 32k max**|\n",
        "| [Llama-3-SthenoMaidBlackroot(8b)](https://huggingface.co/mradermacher/L3-SthenoMaidBlackroot-8B-V1-GGUF)|Llama3|Stheno + Blackroot(story lora) + Maid(no robot dataset) give somewhat more natural feeling compare to normal stheno|**8k base/ 32k max**|\n",
        "| [Llama-3-Umbral-Mind(8b)](https://huggingface.co/mradermacher/L3-Umbral-Mind-RP-v2.0-8B-GGUF)|Llama3|Specialize in mental illness/suicide bot, has negativity bias so it could works great on non-con bot too. also the mental illness part makes the inner monolog more detailed|**8k base/ 32k max**|\n",
        "| [Llama-3-Hathor-Stable(8b)](https://huggingface.co/mradermacher/Hathor_Stable-v0.2-L3-8B-GGUF)|Llama3|Trained on lightnovel data. could be nice for people that into japanese literature|**8k base/ 32k max**|\n",
        "| [Llama-3-Chara-Alpha(8b)](https://huggingface.co/mradermacher/L3-8B-Chara-v1-Alpha-GGUF)|Llama3|Model designed for multiple character rp. Work best on bot that has 2-5 character, but iq is tanked on 1v1 rp|**8k base/ 32k max**|\n",
        "| [Llama-3-Lunaris](https://huggingface.co/bartowski/L3-8B-Lunaris-v1-GGUF)|Llama3|Kinda like stheno but more verbose|**8k base/ 32k max**|\n",
        "| [Llama-3-Hathor-Sofit(8b)](https://huggingface.co/mradermacher/Hathor_Sofit-L3-8B-v1-GGUF)|Llama3|Final version of Hathor, very stable quality and not yapping too much|**8k base/ 32k max**|\n",
        "| [Llama-3.1-Dark-Planet-8-Orbs(8b)](https://huggingface.co/DavidAU/L3-Dark-Planet-8B-V2-Eight-Orbs-Of-Power-GGUF)|Llama3|Negative bias model, good for dark theme, horror or angst. has very unique prose style, you either like it or hate it|**8k base/ 32k max**|\n",
        "| [Llama-3.1-DarkIdol](https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF)|Llama3|All rounder finetunes for llama3.1, not very obedient to violence|**8k base/ 32k max**|\n",
        "| [Yodayo-Nephra(8b)](https://huggingface.co/Marcus-Arcadius/nephra_v1.0-Q8_0-GGUF)|Llama3|For people that feels nostalgic of old yodayo model|**8k base/ 32k max**|\n",
        "| [Gemma2-SPPO(9b)](https://huggingface.co/mradermacher/Gemma-2-9B-It-SPPO-Iter3-i1-GGUF)|Gemma2|Trained with SPPO method to makes it a bit smarter than normal gemma, but also reinforce the refusal too. (not hard to crack with jailbreak)|**8k base/ 24k max**|\n",
        "| [Tarnished(9b)](https://huggingface.co/mradermacher/tarnished-9b-GGUF)|Gemma2|Gemma trained for adventure rp.|**8k base/ 24k max**|\n",
        "| [Gemma2-Daybreak(9b)](https://huggingface.co/mradermacher/gemma2-9B-daybreak-v0.5-i1-GGUF)|Gemma2|Daybreak is a storywriting lora that contained no gpt-slop (barely audible, shivers down spines, etc.) merged with Gemma2 to toned down its shakespearean spirit|**8k base/ 24k max**|\n",
        "| [Gemma2-Sunfall(9b)](https://huggingface.co/mradermacher/gemma2-9B-sunfall-v0.5.2-i1-GGUF)|Gemma2|Very good for bot as scenario. Sunfall models needs strict custom prompt. [please add diamond's law, tag, system prompt into custom prompt box](https://huggingface.co/crestf411/gemma2-9B-sunfall-v0.5.2?not-for-all-audiences=true)|**8k base/ 24k max**|\n",
        "| [Gemma2-Ataraxy(9b)](https://huggingface.co/bartowski/Gemma-2-Ataraxy-9B-GGUF)|Gemma2|Beat the shit out of gpt4 in creative writing benchmark. Just like all gemma model, lean on sfw side|**8k base/ 24k max**|\n",
        "| [Fimbulvetr2(11b)](https://huggingface.co/Lewdiculous/Fimbulvetr-11B-v2-GGUF-IQ-Imatrix)|Alpaca / Vicuna|Best 11b for newbie. works right out of the box.|**8k base/ 24k max**|\n",
        "| [Fimbulvetr-Kuro-Lotus(11b)](https://huggingface.co/Bakanayatsu/Fimbulvetr-Kuro-Lotus-10.7B-GGUF-imatrix)|Alpaca / Vicuna|Merge of the best 2 11b rn|**8k base/ 32k max**|\n",
        "| [Kaiju(11b)](https://huggingface.co/Himitsui/Kaiju-11B-GGUF)|Alpaca / Vicuna|One of the best 11b rn|**8k base/ 32k max**|\n",
        "| [Fimbulvetr-Holodeck-Erebus-Westlake(11b)](https://huggingface.co/PJMixers/Fimbulvetr-Holodeck-Erebus-Westlake-10.7B-GGUF)|Alpaca / Vicuna|Somewhat more coherent than normal fimbul|**8k base/ 32k max**|\n",
        "| [MoistralV3(11b)](https://huggingface.co/TheDrummer/Moistral-11B-v3-GGUF)|Alpaca / Vicuna|Finetuned fimbulvetr to have more \"MOIST\"|**8k base/ 32k max**|\n",
        "| [Lumimaid(12b)](https://huggingface.co/mradermacher/Lumimaid-v0.2-12B-i1-GGUF)|Mistral|Another attempt to make \"Claude opus at home\", although the brain is nowhere near that level. Word choice and behavior is quite close.|**8k base/ 24k max**|\n",
        "| [Mini-Magnum(12b)](https://huggingface.co/InferenceIllusionist/mini-magnum-12b-v1.1-iMat-GGUF)|Mistral|Trained on claude opus's response. Good prose, creativity and smartness. Also make use of character's trait/clothing really well.|**8k base/ 24k max**|\n",
        "| [Nemomix(12b)](https://huggingface.co/mradermacher/Nemomix-v3.0-12B-i1-GGUF)|Mistral|All Mistral-nemo finetunes blend together. Scored #2 on benchmark|**8k base/ 24k max**|\n",
        "| [Celeste(12b)](https://huggingface.co/QuantFactory/Celeste-12B-V1.6-GGUF)|Chatml|First message have very big impact on this model. Try using this on bot with good starting message, so it can cook the same level of result. May need creator's suggested custom prompt (check in model's link)|**8k base/ 24k max**|\n",
        "| [Lyra(12b)](https://huggingface.co/Lewdiculous/MN-12B-Lyra-v4-GGUF-IQ-Imatrix)|Mistral / Chatml|Unhinged model that has very good spatial awareness. Also very aggressive like sonnet, if you mention anything nsfw it will jump on you|**8k base/ 24k max**|\n",
        "| [Guns-and-roses(12b)](https://huggingface.co/Reiterate3680/guns-and-roses-r1-GGUF)|Mistral|Developed on top of magnum. deliver good prose and brain, Specialize in character that has unusual anatomy such as furry,demihuman or monster|**8k base/ 24k max**|\n",
        "| [Magnum(12b)](https://huggingface.co/anthracite-org/magnum-v4-12b-gguf)|Chatml|Magnum that is no longer mini. Fix lot of issue from mini-magnum like positivity-bias, improve character detail handling.|**8k base/ 24k max**|\n",
        "| [Starcannon(12b)](https://huggingface.co/mradermacher/MN-12B-Starcannon-v3-i1-GGUF)|Mistral/Chatml|Merge between Magnum and Celeste to makes more unhinged Magnum. Mistral and ChatML gives different vibe of response, test and try by yourself|**8k base/ 24k max**|\n",
        "| [Rocinante(12b)](https://huggingface.co/TheDrummer/Rocinante-12B-v1.1-GGUF)|Chatml(unhinged)/Mistral(vulgar)|#5 on writing style leaderboard, but its really unhinged. Can't describe this one. you either like it or hate it. Just like moistral|**8k base/ 24k max**|\n",
        "| [Chronos Gold(12b)](https://huggingface.co/mradermacher/Chronos-Gold-12B-1.0-i1-GGUF)|Chatml|May need a bit more handholding than other model. But once you do, it nailed smut and being more creative than openai, with lower amount of shakespearean|**8k base/ 24k max**|\n",
        "| [Mag-Mell(12b)](https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF)|Chatml/Mistral|Nailed the natural feeling for prose, also very good at world building and adventure stuff. |**8k base/ 24k max**|\n",
        "| [Violet Twilight(12b)](https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF)|Chatml/Mistral|Low amount of gpt slop. Dialogue feels very in-character. it also add detail into the scene to makes it more unpredictable|**8k base/ 24k max**|\n",
        "| [Halide(12b)](https://huggingface.co/mradermacher/MN-Halide-12b-v1.0-i1-GGUF)|Chatml/Mistral|Good candidate for slowburn. follow instruction and handholding very well, but at the same time, it has horny resistant that not too obedient to nsfw stuff.|**8k base/ 24k max**|\n",
        "| [Stellar Odyssy(12b)](https://huggingface.co/mradermacher/Stellar-Odyssey-12b-v0.0-i1-GGUF)|Chatml|Not very strict at keeping character definition. But it give more creative response, could be alternative to violet twilight|**8k base/ 24k max**|\n",
        "| [MadMix(12b)](https://huggingface.co/mradermacher/MadMix-Unleashed-12B-i1-GGUF)|Chatml/Mistral|Nemomix with more negative bias and less gptism. |**8k base/ 24k max**|\n",
        "| [DarkPlanet(12b)](https://huggingface.co/DavidAU/MN-Dark-Planet-TITAN-12B-GGUF)|Chatml/Mistral|Bigger dark planet, like more temp (>1). More intense horror, dark and angst output|**8k base/ 24k max**|\n",
        "| [UnslopNemoV4.1(12b)](https://huggingface.co/TheDrummer/UnslopNemo-12B-v4.1-GGUF)|Metharme/Chatml|[After 6 months, humankind finally win the war against ministration of the voice barely above whisper that send shiver down your spine](https://www.reddit.com/r/SillyTavernAI/comments/1gafnom/the_absolute_final_call_to_arms_project_unslop/). Use metharme for more effective.|**8k base/ 24k max**|\n",
        "| [Violet Lotus(12b)](https://huggingface.co/QuantFactory/MN-Violet-Lotus-12B-GGUF)|Chatml/Mistral|Pretty much lyra with added plot twist/unpredictable element from violet twilight|**8k base/ 24k max**|\n",
        "| [Abomination Science(12b)](https://huggingface.co/mradermacher/AbominationScience-12B-v4-i1-GGUF)|Chatml/Mistral|Monster merge that contain almost all model above. Theoretically could handle more situation.|**8k base/ 24k max**|\n",
        "| [DarkAtom(12b)](https://huggingface.co/mradermacher/DarkAtom-12B-v3-i1-GGUF)|Chatml/Mistral|Same as Abomination Science. But with more model merged.|**8k base/ 24k max**|\n",
        "| [CaptainErisViolet(12b)](https://huggingface.co/QuantFactory/Captain-Eris_Violet-V0.420-12B-GGUF)|Chatml/Mistral|Another \"violet\" model which has unpredictable behavior. This one focus on emotional and thinking process. Although not very good at keeping character detail|**8k base/ 24k max**|\n",
        "| [Ink(12b)](https://huggingface.co/allura-org/MN-12b-RP-Ink-GGUF)|Chatml/Mistral|Nastiest model in this list. dataset used to making it is disclosed to not accidentally kill person who dare looking at it|**8k base/ 24k max**|\n",
        "| [Wayfarer(12b)](https://huggingface.co/LatitudeGames/Wayfarer-12B-GGUF)|Chatml/Mistral|AI dungeon official model. if you like DND stuff, this could be your best model|**8k base/ 24k max**|\n",
        "| [TieFighter(13b)](https://huggingface.co/KoboldAI/LLaMA2-13B-Tiefighter-GGUF)|Alpaca / Vicuna|Maybe the best 13b model for plug and play. good at nsfw, good at keeping character persona. no need to setting much|**4k base / 6k max**|\n",
        "| [Psyfighter(13b)](https://huggingface.co/TheBloke/Psyfighter-13B-GGUF)|Alpaca / Vicuna|Love child of Tiefighter and Psymed. Big brain like Tiefigher, smart like PsyMed.|**4k base / 6k max**|\n",
        "| [Psyfighter2(13b)](https://huggingface.co/KoboldAI/LLaMA2-13B-Psyfighter2-GGUF)|Alpaca / Vicuna|Psyfighter but newer, if you don't like normal psyfighter. this could works|**4k base / 6k max**|\n",
        "| [PsyMedRP(13b)](https://huggingface.co/Undi95/PsyMedRP-v1-13B-GGUF)|Alpaca / Vicuna|Fancy prose, very large vocabulary. Selling point are psych/anatomy/spatial understanding|**4k base / 6k max**|\n",
        "| [EstopianMaid(13b)](https://huggingface.co/KatyTheCutie/EstopianMaid-13B-GGUF)|Alpaca / Vicuna|Noromaid + Estopia good at sticking to the character card. maintains coherency in a setting with multiple characters. Able to create new scenario. This is story writing model, so it will give pretty long answer|**4k base / 6k max**|\n",
        "| [Noromaid0.1(13b)](https://huggingface.co/NeverSleep/Noromaid-13b-v0.1.1-GGUF)|Alpaca / Vicuna|pretty good model for chats. It has good reasoning but requires some tinkering with temps to get most out of it. It follows pretty good markdown structure, could be good for rpg style bot with stats|**4k base / 6k max**|\n",
        "| [EVA-Qwen2.5 (14b)](https://huggingface.co/bartowski/EVA-Qwen2.5-14B-v0.0-GGUF)|Chatml|Very good unhinged model, because its 14b. it feels smarter than lyra. |**8k base/ 16k max**|\n",
        "| [EVA-Tissint(14b)](https://huggingface.co/mradermacher/EVA-Tissint-v1.2-14B-i1-GGUF)|Chatml|More stable EVA.|**8k base/ 16k max**|\n",
        "| [Sugarquill(14b)](https://huggingface.co/Triangle104/TQ2.5-14B-Sugarquill-v1-Q5_K_M-GGUF)|Chatml|Tuned for adventure/storywriting. kinda like mag-mell but bigger|**8k base/ 16k max**|\n",
        "| [Freya(14b)](https://huggingface.co/mradermacher/14B-Qwen2.5-Freya-x1-i1-GGUF)|Chatml|Kinda smart and proseful. And could work without tinkering much with sampler|**8k base/ 16k max**|\n",
        "| [Kunou(14b)](https://huggingface.co/mradermacher/14B-Qwen2.5-Kunou-v1-GGUF)|Chatml|Horny and proseful. You can think of it as bigger Starcannon, or smaller Hanami|**8k base/ 16k max**|\n",
        "| [Sailor2(14b)](https://huggingface.co/mradermacher/Sailor2-14B-GGUF)|Chatml|SEA multilingual model. Can speak Burmese, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese. Not a roleplaying model but can do it to some extent|**Only 8k**|\n",
        "| [Sailor2-Chat(14b)](https://huggingface.co/mradermacher/Sailor2-14B-Chat-GGUF)|Chatml|Same as sailor2, but shorter response like cai|**Only 8k**|\n",
        "| [Deepseek-Kunou(14b)](https://huggingface.co/mradermacher/Deepseeker-Kunou-Qwen2.5-14b-i1-GGUF)|Chatml|Same as kunou, but with added brain power from Deepseek|**8k base/ 16k max**|\n",
        "| [L3.1 OpenCrystal(15b)](https://huggingface.co/mradermacher/OpenCrystal-15B-L3-v2-i1-GGUF)|Llama3|Even more experimental upscale of Llama3.1 model. Works like 12b one, but less hallucination and slower|**8k base/ 12k max**|\n",
        "| [Cydonia(22b)](https://huggingface.co/MarsupialAI/Cydonia-22B-v1_iMat_GGUF)|Chatml/Mistral/Metharme|Good prose and word choices with less gptism. Location awareness okayish, it not teleport you often unless your trying to confuse it.|**Only 8k**|\n",
        "| [Magnum(22b)](https://huggingface.co/mradermacher/magnum-v4-22b-i1-GGUF)|Chatml|Bigger magnum, same good stuff from magnum. But smarter due to being 22b|**Only 8k**|\n",
        "| [Sorcerer(22b)](https://huggingface.co/Quant-Cartel/SorcererLM-22B-iMat-GGUF)|Mistral|Try to mimic the weight of Sorcerer 8x22b (which is rp tuned of wizard 8x22b). If you enjoy wizard, this could be good|**Only 8k**|\n",
        "\n",
        "13b models are a bit outdated by today standard. but still good at its own niche. Unless you tried to take advantage of those niche, I suggest using 12b more.\n",
        "\n",
        "###*Using more than base context may make answer incoherent sometimes. especially in janitor\n",
        "\n",
        "---\n",
        "\n",
        "More B = More capable, but it doesn't mean smaller model are bad. as smaller model got benefit of better memory due to larger context size instead.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Instruction\n",
        "1. select model\n",
        "2. select context\n",
        "3. select layer (no need to change if you use model in dropdown)\n",
        "4. select instruct preset (correct preset improve coherency for the model. look at the table above for reference)\n",
        "5. run koboldcpp cell (first time running can takes up to 10 minutes, as it need to download koboldcpp and model. it will takes around 1 min next time)\n",
        "6. once log give you http://xxx-xxx-xxx.trycloudflare.com/ thingy, copy cloudflare link to use as kobold url in this format.\n",
        "\n",
        "```\n",
        "https://example.trycloudflare.com/v1/chat/completions\n",
        "```\n",
        "\n",
        "---\n",
        "*In case you use localtunnel as provider, you need to open the link and verify it with colab ip before using. once log give you http://localhost:5001/ thingy, scroll up a bit and copy loca.lt link to use as kobold url instead.*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "if you want to use specific model, you can copy its download link from huggingface and paste in Model field instead of choosing from drop down. but you can run 13b model at most. import anything larger likely will cause error.\n",
        "\n",
        "---\n",
        "\n",
        "if log say anything about cuda. you may need to change email, cause your current email likely run out of daily gpu quota. using max context will use daily gpu quota faster\n"
      ],
      "metadata": {
        "id": "pf4AQOYgTB2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b>v-- If you play on mobile, tap this to open music player and play the white noise to keep tab running in the background. or google will kill your api\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start block below (Uses only 13MB of data)</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "61KnkAi6CrHp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJS9i_Dltv8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e4c19e-a19b-4ca9-cee9-d719a1b558e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading KoboldCpp, please wait...\n",
            "--2025-02-22 04:02:18--  https://kcpplinux.concedo.workers.dev/\n",
            "Resolving kcpplinux.concedo.workers.dev (kcpplinux.concedo.workers.dev)... 104.21.71.155, 172.67.145.201, 2606:4700:3033::6815:479b, ...\n",
            "Connecting to kcpplinux.concedo.workers.dev (kcpplinux.concedo.workers.dev)|104.21.71.155|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/LostRuins/koboldcpp/releases/latest/download/koboldcpp-linux-x64-cuda1210 [following]\n",
            "--2025-02-22 04:02:18--  https://github.com/LostRuins/koboldcpp/releases/latest/download/koboldcpp-linux-x64-cuda1210\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/LostRuins/koboldcpp/releases/download/v1.84.2/koboldcpp-linux-x64-cuda1210 [following]\n",
            "--2025-02-22 04:02:18--  https://github.com/LostRuins/koboldcpp/releases/download/v1.84.2/koboldcpp-linux-x64-cuda1210\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/614637892/c5106f06-5078-4b97-8c08-d4f670970d8b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250222%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250222T040218Z&X-Amz-Expires=300&X-Amz-Signature=2dd847b0b254949ff48cf8a6f1aab30439a0657d96ab2eb61ed13e3921f26081&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dkoboldcpp-linux-x64-cuda1210&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-02-22 04:02:18--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/614637892/c5106f06-5078-4b97-8c08-d4f670970d8b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250222%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250222T040218Z&X-Amz-Expires=300&X-Amz-Signature=2dd847b0b254949ff48cf8a6f1aab30439a0657d96ab2eb61ed13e3921f26081&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dkoboldcpp-linux-x64-cuda1210&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 676669864 (645M) [application/octet-stream]\n",
            "Saving to: ‘dlfile.tmp’\n",
            "\n",
            "dlfile.tmp          100%[===================>] 645.32M  60.5MB/s    in 6.5s    \n",
            "\n",
            "2025-02-22 04:02:25 (99.5 MB/s) - ‘dlfile.tmp’ saved [676669864/676669864]\n",
            "\n",
            "Download Successful\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "31 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "aria2 is already the newest version (1.36.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "oldmodel = https://huggingface.co/kpkiosk66/Sailor2-20B-Chat-Q4_K_S-GGUF/resolve/main/sailor2-20b-chat-q4_k_s.gguf?download=true\n",
            "Download: https://huggingface.co/mradermacher/Sailor2-14B-GGUF/resolve/main/Sailor2-14B.Q4_K_S.gguf?download=true\n",
            "\n",
            "02/22 04:02:31 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "02/22 04:02:31 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200551&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=IG7%7Ephbxu1kCI6oT0qBg-WJ2Anll9CkrogFyBF2e-DCPSBYXYDljOh26kCx-84UpQL9qtmtJ7RbBMzpKb8DmWr9tUCwa4EXr-0TmaJRpSPK5in5lt3Cz7bqGlSILvsHdDuzfl1gJFtQsmrVFljEQhk4rsVjqMbaS6ytg9QMK-%7EjTR2kKAhSc13GCpE16JjnVR4ZjbosKOTaNT-1Uc0BzwLrSRPwa4sXjPKxx3S1PNohj2Okejk1CVmOiG8PMrvIxk8TaCs0qaQmTr9G8JS6Yll0ZxYrPZ-NckZ3sgNarA69Ivk32Ue32wi2o6fm9TlLl%7EPVvEgRfv9OHKnqnsk6BtA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\u001b[0m\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#8 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#9 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#10 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#11 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#12 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#13 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#14 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#16 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#19 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#20 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#21 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#22 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#17 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#14 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#22 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#17 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "\n",
            "02/22 04:02:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#18 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200553&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=rYQOdHheacKsGx%7ErjSvzdULusdKjtAuG0N-Hez9s3tlIcBdddiXGkMs8G-Kx5crmBW9h8lxrPW-8pU-Cd4E5w%7ErYv6frTQgR9lRP7EH73jVp5o-BmicbVSnDvlvkt0dxCQkbTdSgb8uhYysp5Gz7dP%7EiIpAeqCO64Bla-JaL2UluFQoXyOxOP5Fq1hBw5-LRFUjAzY-uiSQH3iG1TQe%7EQX9PaJNIS97X83dQYRG1o418274JYXHVV9WQ4Ai8d5svxFxmbtDsL1201DbAAyZoMWyQsvp1H6Ami8KWL2MuUpejIqJMFu0-85JYX7ADTDcwN3NZyT2TWGJoCGi5bZ%7EW7g__&Key-Pair-Id=K24J24Z295AEI9\n",
            " *** Download Progress Summary as of Sat Feb 22 04:02:37 2025 *** \n",
            "=\n",
            "[#4d73d2 882MiB/12GiB(6%) CN:16 DL:222MiB ETA:53s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            "\u001b[0m\n",
            "02/22 04:02:40 [\u001b[1;32mNOTICE\u001b[0m] CUID#15 - Redirecting to https://cdn-lfs-us-1.hf.co/repos/57/7d/577dbb5f56df6916b6df7cd10fef8a7d9271e1c1c3b027f912c6546c63eb25f9/8a41c21cc962c75e7185b3a1d1d2becedb5ce2f6b68439bd0ee333a4452c4bdb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Sailor2-14B.Q4_K_S.gguf%3B+filename%3D%22Sailor2-14B.Q4_K_S.gguf%22%3B&Expires=1740200560&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDIwMDU2MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzU3LzdkLzU3N2RiYjVmNTZkZjY5MTZiNmRmN2NkMTBmZWY4YTdkOTI3MWUxYzFjM2IwMjdmOTEyYzY1NDZjNjNlYjI1ZjkvOGE0MWMyMWNjOTYyYzc1ZTcxODViM2ExZDFkMmJlY2VkYjVjZTJmNmI2ODQzOWJkMGVlMzMzYTQ0NTJjNGJkYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=MxlHxpz3oSr0xFMXpL2z4ac1Z%7EwmGbTi1bDRwiZY-z86a6kIfiPWjA%7EJOgbWTIX4wjH3-qch3eMSQ9UIl7HikNhovK2BAmlj5O9nz22AAcKahljVEkgcfxjwmfwr6LLcDPzswlH42nF2ZLFjdqtn75oNbG27pnfJSjYpt1aGV5dKpvG7DMVitwDN-WBQfrFf72f2wJHg%7EuhuzrpXILBEhxi3HsbdayPh1ulE1YnAF5tRg93QDoCmfVSuo6-Sq0ZE98X%7E8QjVdUaW8evIIlKYVEYrkJzH1SFGBmn1N9sx-WdBtHm0cUmT5uZJ-d7y34LKsHsMOT%7EDm7aN2BU0cnHB8A__&Key-Pair-Id=K24J24Z295AEI9\n",
            " *** Download Progress Summary as of Sat Feb 22 04:02:43 2025 *** \n",
            "=\n",
            "[#4d73d2 2.2GiB/12GiB(18%) CN:16 DL:230MiB ETA:45s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Sat Feb 22 04:02:48 2025 *** \n",
            "=\n",
            "[#4d73d2 2.9GiB/12GiB(23%) CN:16 DL:177MiB ETA:54s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Sat Feb 22 04:02:55 2025 *** \n",
            "=\n",
            "[#4d73d2 3.5GiB/12GiB(28%) CN:16 DL:99MiB ETA:1m30s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Sat Feb 22 04:03:00 2025 *** \n",
            "=\n",
            "[#4d73d2 4.1GiB/12GiB(33%) CN:16 DL:79MiB ETA:1m46s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Sat Feb 22 04:03:06 2025 *** \n",
            "=\n",
            "[#4d73d2 5.4GiB/12GiB(43%) CN:16 DL:153MiB ETA:46s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Sat Feb 22 04:03:12 2025 *** \n",
            "=\n",
            "[#4d73d2 6.6GiB/12GiB(53%) CN:16 DL:229MiB ETA:25s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Sat Feb 22 04:03:17 2025 *** \n",
            "=\n",
            "[#4d73d2 7.4GiB/12GiB(59%) CN:16 DL:174MiB ETA:29s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Sat Feb 22 04:03:22 2025 *** \n",
            "=\n",
            "[#4d73d2 8.3GiB/12GiB(66%) CN:16 DL:152MiB ETA:27s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Sat Feb 22 04:03:28 2025 *** \n",
            "=\n",
            "[#4d73d2 9.6GiB/12GiB(77%) CN:16 DL:181MiB ETA:15s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Sat Feb 22 04:03:34 2025 *** \n",
            "=\n",
            "[#4d73d2 10GiB/12GiB(83%) CN:16 DL:168MiB ETA:12s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Sat Feb 22 04:03:40 2025 *** \n",
            "=\n",
            "[#4d73d2 11GiB/12GiB(93%) CN:16 DL:228MiB ETA:3s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            "\u001b[0m\n",
            "02/22 04:03:44 [\u001b[1;32mNOTICE\u001b[0m] Download complete: /content/model.gguf\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "4d73d2|\u001b[1;32mOK\u001b[0m  |   176MiB/s|/content/model.gguf\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "***\n",
            "Welcome to KoboldCpp - Version 1.84.2\n",
            "cloudflared-linux-amd64 already exists, using existing file.\n",
            "Attempting to start tunnel thread...\n",
            "Loading Chat Completions Adapter: /content/instruct.json\n",
            "Chat Completions Adapter Loaded\n",
            "Initializing dynamic library: koboldcpp_cublas.so\n",
            "==========\n",
            "Starting Cloudflare Tunnel for Linux, please wait...\n",
            "Namespace(admin=False, admindir='', adminpassword=None, analyze='', benchmark=None, blasbatchsize=512, blasthreads=1, chatcompletionsadapter='instruct.json', config=None, contextsize=8192, debugmode=-1, draftamount=8, draftgpulayers=999, draftgpusplit=None, draftmodel='', failsafe=False, flashattention=True, forceversion=0, foreground=False, gpulayers=99, highpriority=False, hordeconfig=None, hordegenlen=0, hordekey='', hordemaxctx=0, hordemodelname='koboldcpp/Sailor2-14B', hordeworkername='', host='', ignoremissing=False, launch=False, lora=None, mmproj='', model=[], model_param='model.gguf', moeexperts=-1, multiplayer=False, multiuser=1, noavx2=False, noblas=False, nocertify=False, nofastforward=False, nommap=False, nomodel=False, noshift=False, onready='', password=None, port=5001, port_param=5001, preloadstory='', prompt='', promptlimit=100, quantkv=0, quiet=True, remotetunnel=True, ropeconfig=[0.0, 10000.0], sdclamped=0, sdclipg='', sdclipl='', sdconfig=None, sdlora='', sdloramult=1.0, sdmodel='', sdnotile=False, sdquant=False, sdt5xxl='', sdthreads=0, sdvae='', sdvaeauto=False, showgui=False, skiplauncher=False, smartcontext=False, ssl=None, tensor_split=None, threads=1, ttsgpu=False, ttsmaxlen=4096, ttsmodel='', ttsthreads=0, ttswavtokenizer='', unpack='', useclblast=None, usecpu=False, usecublas=['0', 'mmq'], usemlock=False, usemmap=False, usevulkan=None, version=False, visionmaxres=1024, websearch=False, whispermodel='')\n",
            "==========\n",
            "Loading Text Model: /content/model.gguf\n",
            "\n",
            "The reported GGUF Arch is: qwen2\n",
            "Arch Category: 5\n",
            "\n",
            "---\n",
            "Identified as GGUF model: (ver 6)\n",
            "Attempting to Load...\n",
            "---\n",
            "Using automatic RoPE scaling for GGUF. If the model has custom RoPE settings, they'll be used directly instead!\n",
            "System Info: AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "---\n",
            "Initializing CUDA/HIP, please wait, the following step may take a few minutes for first launch...\n",
            "---\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 39 key-value pairs and 771 tensors from /content/model.gguf (version GGUF V3 (latest))\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = unknown, may not work\n",
            "print_info: file size   = 12.41 GiB (7.38 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: special tokens cache size = 22\n",
            "load: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 64\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 13368\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 32B\n",
            "print_info: model params     = 14.44 B\n",
            "print_info: general.name     = Sailor2 14B\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151643 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = false)\n",
            "load_tensors: relocated tensors: 1 of 771\n",
            "load_tensors: offloading 64 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 65/65 layers to GPU\n",
            "load_tensors:          CPU model buffer size =   333.84 MiB\n",
            "load_tensors:        CUDA0 model buffer size = 12376.12 MiB\n",
            "load_all_data: no device found for buffer type CPU for async uploads\n",
            "load_all_data: using async uploads for device CUDA0, buffer type CUDA0, backend CUDA0\n",
            "................................................................................................\n",
            "Automatic RoPE Scaling: Using model internal value.\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 8448\n",
            "llama_init_from_model: n_ctx_per_seq = 8448\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 1\n",
            "llama_init_from_model: freq_base     = 1000000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_pre_seq (8448) > n_ctx_train (4096) -- possible training context overflow\n",
            "llama_kv_cache_init: kv_size = 8448, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  2112.00 MiB\n",
            "llama_init_from_model: KV self size  = 2112.00 MiB, K (f16): 1056.00 MiB, V (f16): 1056.00 MiB\n",
            "llama_init_from_model:  CUDA_Host  output buffer size =     0.58 MiB\n",
            "llama_init_from_model:      CUDA0 compute buffer size =   312.75 MiB\n",
            "llama_init_from_model:  CUDA_Host compute buffer size =    24.51 MiB\n",
            "llama_init_from_model: graph nodes  = 1991\n",
            "llama_init_from_model: graph splits = 2\n",
            "Load Text Model OK: True\n",
            "Embedded KoboldAI Lite loaded.\n",
            "Embedded API docs loaded.\n",
            "======\n",
            "Active Modules: TextGeneration\n",
            "Inactive Modules: ImageGeneration VoiceRecognition MultimodalVision NetworkMultiplayer ApiKeyPassword WebSearchProxy TextToSpeech AdminControl\n",
            "Enabled APIs: KoboldCppApi OpenAiApi OllamaApi\n",
            "Your remote Kobold API can be found at https://thumb-constitute-virgin-indicator.trycloudflare.com/api\n",
            "Your remote OpenAI Compatible API can be found at https://thumb-constitute-virgin-indicator.trycloudflare.com/v1\n",
            "======\n",
            "Your remote tunnel is ready, please connect to https://thumb-constitute-virgin-indicator.trycloudflare.com\n",
            "\n",
            "(Note: Non-default sampler_order detected. Recommended sampler values are [6,0,1,3,4,2,5]. This message will only show once per session.)\n",
            "Token streaming was interrupted or aborted!\n",
            "[Errno 32] Broken pipe\n",
            "\n",
            "[04:06:31] CtxLimit:2033/8192, Amt:125/1202, Init:0.03s, Process:2.96s (1.6ms/T = 644.59T/s), Generate:9.15s (73.2ms/T = 13.66T/s), Total:12.11s (10.32T/s)\n",
            "Generation Aborted\n",
            "\n",
            "[04:07:30] CtxLimit:7019/8192, Amt:156/1202, Init:0.13s, Process:11.36s (1.8ms/T = 562.12T/s), Generate:13.33s (85.5ms/T = 11.70T/s), Total:24.69s (6.32T/s)Token streaming was interrupted or aborted!\n",
            "[Errno 32] Broken pipe\n",
            "\n",
            "Generation Aborted\n",
            "\n",
            "[04:08:15] CtxLimit:7042/8192, Amt:181/1202, Init:0.08s, Process:0.13s (12.2ms/T = 82.09T/s), Generate:15.31s (84.6ms/T = 11.82T/s), Total:15.44s (11.72T/s)Token streaming was interrupted or aborted!\n",
            "[Errno 32] Broken pipe\n",
            "\n",
            "Generation Aborted\n",
            "\n",
            "[04:12:58] CtxLimit:3251/8192, Amt:259/1042, Init:0.14s, Process:4.68s (1.6ms/T = 638.60T/s), Generate:19.47s (75.2ms/T = 13.30T/s), Total:24.15s (10.72T/s)\n",
            "[04:13:43] CtxLimit:3237/8192, Amt:245/1042, Init:0.02s, Process:0.05s (47.0ms/T = 21.28T/s), Generate:18.31s (74.7ms/T = 13.38T/s), Total:18.36s (13.35T/s)\n",
            "[04:14:17] CtxLimit:3228/8192, Amt:236/1042, Init:0.02s, Process:0.05s (47.0ms/T = 21.28T/s), Generate:17.76s (75.3ms/T = 13.29T/s), Total:17.81s (13.25T/s)\n",
            "[04:16:56] CtxLimit:3383/8192, Amt:129/1042, Init:0.04s, Process:0.28s (1.9ms/T = 528.17T/s), Generate:9.97s (77.3ms/T = 12.93T/s), Total:10.26s (12.58T/s)\n",
            "[04:17:14] CtxLimit:3322/8192, Amt:68/1042, Init:0.03s, Process:0.05s (48.0ms/T = 20.83T/s), Generate:5.21s (76.6ms/T = 13.06T/s), Total:5.25s (12.94T/s)\n",
            "[04:17:34] CtxLimit:3426/8192, Amt:172/1042, Init:0.03s, Process:0.05s (49.0ms/T = 20.41T/s), Generate:13.24s (77.0ms/T = 12.99T/s), Total:13.29s (12.94T/s)\n",
            "[04:18:05] CtxLimit:3448/8192, Amt:194/1042, Init:0.02s, Process:0.05s (48.0ms/T = 20.83T/s), Generate:14.84s (76.5ms/T = 13.08T/s), Total:14.88s (13.03T/s)\n",
            "[04:18:32] CtxLimit:3399/8192, Amt:145/1042, Init:0.02s, Process:0.05s (48.0ms/T = 20.83T/s), Generate:11.13s (76.8ms/T = 13.02T/s), Total:11.18s (12.97T/s)\n",
            "[04:19:32] CtxLimit:3426/8192, Amt:172/1042, Init:0.03s, Process:0.06s (55.0ms/T = 18.18T/s), Generate:13.09s (76.1ms/T = 13.14T/s), Total:13.14s (13.09T/s)\n",
            "[04:20:32] CtxLimit:3361/8192, Amt:107/1042, Init:0.02s, Process:0.05s (48.0ms/T = 20.83T/s), Generate:8.12s (75.9ms/T = 13.18T/s), Total:8.17s (13.10T/s)\n",
            "[04:26:51] CtxLimit:2505/8192, Amt:89/1042, Init:0.06s, Process:3.75s (1.6ms/T = 642.72T/s), Generate:6.65s (74.7ms/T = 13.39T/s), Total:10.39s (8.56T/s)\n",
            "[04:27:59] CtxLimit:2626/8192, Amt:89/1042, Init:0.03s, Process:0.16s (5.0ms/T = 201.26T/s), Generate:6.52s (73.2ms/T = 13.65T/s), Total:6.68s (13.33T/s)\n",
            "[04:28:14] CtxLimit:2639/8192, Amt:102/1042, Init:0.02s, Process:0.05s (46.0ms/T = 21.74T/s), Generate:7.46s (73.1ms/T = 13.67T/s), Total:7.51s (13.59T/s)\n",
            "[04:29:54] CtxLimit:3579/8192, Amt:1042/1042, Init:0.02s, Process:0.05s (46.0ms/T = 21.74T/s), Generate:78.85s (75.7ms/T = 13.21T/s), Total:78.90s (13.21T/s)\n",
            "[04:36:18] CtxLimit:2985/8192, Amt:448/1042, Init:0.02s, Process:0.05s (46.0ms/T = 21.74T/s), Generate:32.95s (73.6ms/T = 13.59T/s), Total:33.00s (13.58T/s)\n",
            "[04:38:53] CtxLimit:3579/8192, Amt:1042/1042, Init:0.11s, Process:0.05s (46.0ms/T = 21.74T/s), Generate:79.07s (75.9ms/T = 13.18T/s), Total:79.12s (13.17T/s)\n",
            "[04:39:40] CtxLimit:2636/8192, Amt:99/1042, Init:0.12s, Process:0.05s (47.0ms/T = 21.28T/s), Generate:7.62s (77.0ms/T = 12.99T/s), Total:7.67s (12.91T/s)\n",
            "[04:41:12] CtxLimit:3579/8192, Amt:1042/1042, Init:0.11s, Process:0.05s (48.0ms/T = 20.83T/s), Generate:81.08s (77.8ms/T = 12.85T/s), Total:81.12s (12.84T/s)\n",
            "[04:43:13] CtxLimit:3579/8192, Amt:1042/1042, Init:0.07s, Process:0.05s (46.0ms/T = 21.74T/s), Generate:82.16s (78.8ms/T = 12.68T/s), Total:82.21s (12.68T/s)\n",
            "[04:45:07] CtxLimit:2758/8192, Amt:221/1042, Init:0.12s, Process:0.05s (47.0ms/T = 21.28T/s), Generate:16.38s (74.1ms/T = 13.50T/s), Total:16.42s (13.46T/s)\n",
            "[04:46:39] CtxLimit:2866/8192, Amt:127/1042, Init:0.13s, Process:0.23s (2.1ms/T = 467.53T/s), Generate:9.35s (73.6ms/T = 13.59T/s), Total:9.58s (13.26T/s)\n",
            "[04:51:35] CtxLimit:3059/8192, Amt:132/1042, Init:0.06s, Process:0.24s (2.0ms/T = 493.88T/s), Generate:9.76s (74.0ms/T = 13.52T/s), Total:10.01s (13.19T/s)\n",
            "[04:52:57] CtxLimit:3264/8192, Amt:134/1042, Init:0.07s, Process:0.20s (2.4ms/T = 421.57T/s), Generate:10.03s (74.8ms/T = 13.36T/s), Total:10.23s (13.10T/s)\n",
            "[04:56:25] CtxLimit:3290/8192, Amt:160/1042, Init:0.06s, Process:0.05s (48.0ms/T = 20.83T/s), Generate:11.92s (74.5ms/T = 13.42T/s), Total:11.97s (13.36T/s)\n",
            "[04:57:44] CtxLimit:3384/8192, Amt:46/1042, Init:0.13s, Process:0.35s (1.7ms/T = 582.13T/s), Generate:3.56s (77.5ms/T = 12.91T/s), Total:3.91s (11.76T/s)\n",
            "[04:58:02] CtxLimit:3423/8192, Amt:85/1042, Init:0.09s, Process:0.05s (48.0ms/T = 20.83T/s), Generate:6.39s (75.2ms/T = 13.30T/s), Total:6.44s (13.20T/s)\n",
            "[04:58:25] CtxLimit:3529/8192, Amt:191/1042, Init:0.07s, Process:0.05s (49.0ms/T = 20.41T/s), Generate:14.41s (75.5ms/T = 13.25T/s), Total:14.46s (13.21T/s)\n",
            "[04:58:51] CtxLimit:3404/8192, Amt:66/1042, Init:0.08s, Process:0.05s (48.0ms/T = 20.83T/s), Generate:4.96s (75.2ms/T = 13.30T/s), Total:5.01s (13.17T/s)\n",
            "[04:59:20] CtxLimit:3474/8192, Amt:46/1042, Init:0.07s, Process:0.22s (2.5ms/T = 392.69T/s), Generate:3.45s (75.1ms/T = 13.32T/s), Total:3.67s (12.52T/s)\n",
            "[04:59:37] CtxLimit:3539/8192, Amt:111/1042, Init:0.07s, Process:0.05s (48.0ms/T = 20.83T/s), Generate:8.32s (75.0ms/T = 13.34T/s), Total:8.37s (13.26T/s)\n",
            "[05:00:45] CtxLimit:3755/8192, Amt:139/1042, Init:0.07s, Process:0.20s (2.2ms/T = 446.70T/s), Generate:10.57s (76.0ms/T = 13.15T/s), Total:10.77s (12.91T/s)\n",
            "[05:01:19] CtxLimit:3664/8192, Amt:48/1042, Init:0.13s, Process:0.05s (50.0ms/T = 20.00T/s), Generate:3.68s (76.7ms/T = 13.03T/s), Total:3.73s (12.86T/s)\n",
            "[05:03:06] CtxLimit:3702/8192, Amt:86/1042, Init:0.07s, Process:0.05s (49.0ms/T = 20.41T/s), Generate:6.46s (75.1ms/T = 13.31T/s), Total:6.51s (13.21T/s)\n",
            "[05:05:41] CtxLimit:3876/8192, Amt:92/1042, Init:0.08s, Process:0.31s (1.9ms/T = 524.27T/s), Generate:6.97s (75.7ms/T = 13.20T/s), Total:7.28s (12.64T/s)\n",
            "[05:06:01] CtxLimit:3873/8192, Amt:89/1042, Init:0.07s, Process:0.14s (3.2ms/T = 312.50T/s), Generate:6.75s (75.8ms/T = 13.19T/s), Total:6.89s (12.92T/s)\n",
            "[05:06:27] CtxLimit:3843/8192, Amt:59/1042, Init:0.07s, Process:0.05s (49.0ms/T = 20.41T/s), Generate:4.46s (75.5ms/T = 13.24T/s), Total:4.50s (13.10T/s)\n",
            "[05:07:20] CtxLimit:4040/8192, Amt:96/1042, Init:0.07s, Process:0.33s (2.1ms/T = 468.09T/s), Generate:7.34s (76.5ms/T = 13.07T/s), Total:7.67s (12.51T/s)\n",
            "[05:07:39] CtxLimit:4041/8192, Amt:97/1042, Init:0.07s, Process:0.05s (49.0ms/T = 20.41T/s), Generate:7.41s (76.4ms/T = 13.09T/s), Total:7.46s (13.01T/s)\n",
            "[05:07:53] CtxLimit:4047/8192, Amt:103/1042, Init:0.07s, Process:0.05s (49.0ms/T = 20.41T/s), Generate:7.86s (76.3ms/T = 13.10T/s), Total:7.91s (13.02T/s)\n",
            "[05:08:05] CtxLimit:4030/8192, Amt:86/1042, Init:0.08s, Process:0.05s (49.0ms/T = 20.41T/s), Generate:6.60s (76.7ms/T = 13.04T/s), Total:6.65s (12.94T/s)Exception ignored in: <module 'threading' from '/tmp/_MEIBoP075/threading.pyc'>\n",
            "Traceback (most recent call last):\n",
            "  File \"threading.py\", line 1388, in _shutdown\n",
            "KeyboardInterrupt: \n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# @title <b>v--Select model and then click this to start Koboldcpp (takes around 2-3 min for link to be generated)</b> {\"display-mode\":\"form\"}\n",
        "import os\n",
        "import time\n",
        "if not os.path.isfile(\"/opt/bin/nvidia-smi\"):\n",
        "  raise RuntimeError(\"⚠️Colab did not give you a GPU because you used it to often recently, this can take a few hours before they let you back in. Try again later or subscribe to Colab Pro for immediate access. (or change email if you are really desperate)⚠️\")\n",
        "import time\n",
        "from google.colab import runtime\n",
        "import json\n",
        "import re\n",
        "\n",
        "# @markdown ### DO NOT CLOSE THIS TAB WHILE USING, GOOGLE ANTI AFK WILL KILL API\n",
        "\n",
        "Model = \"Violet Twilight(12b)\"  # @param [\"Kunoichi(7b)\",\"WizardIceLemonTeaRP(7b)\",\"WizardLaker(7b)\",\"StunnaMaid(7b)\",\"LemonKunoichiWizard(7b)\",\"Llama-3-Halu-Blackroot(8k)\",\"Llama-3-Lumimaid(8b)\",\"Llama-3-Daybreak-Lumimaid(8b)\",\"Llama-3-Stheno(8b)\",\"Llama-3-Stheno-ULTRA(8b)\",\"Llama-3-SthenoMaidBlackroot(8b)\",\"Llama-3-Umbral-Mind(8b)\",\"Llama-3-Hathor-Stable(8b)\",\"Llama-3-Chara-Alpha(8b)\",\"Llama-3-Lunaris(8b)\",\"Llama-3-Hathor-Sofit(8b)\",\"Llama-3.1-Dark-Planet-8-Orbs(8b)\",\"Llama-3.1-DarkIdol(8b)\",\"Yodayo-Nephra(8b)\",\"Gemma2-SPPO(9b)\",\"Tarnished(9b)\",\"Gemma2-Daybreak(9b)\",\"Gemma2-Sunfall(9b)\",\"Gemma2-Ataraxy(9b)\",\"Fimbulvetr2(11b)\",\"Fimbulvetr-Kuro-Lotus(11b)\",\"Kaiju(11b)\",\"Fimbulvetr-Holodeck-Erebus-Westlake(11b)\",\"MoistralV3(11b)\",\"Lumimaid(12b)\",\"Mini-Magnum(12b)\",\"Nemomix(12b)\",\"Celeste(12b)\",\"Lyra(12b)\",\"Guns-and-roses(12b)\",\"Magnum(12b)\",\"Starcannon(12b)\",\"Rocinante(12b)\",\"Chronos Gold(12b)\",\"L3.1 OpenCrystal(12b)\",\"Mag-Mell(12b)\",\"Violet Twilight(12b)\",\"Halide(12b)\",\"Stellar Odyssy(12b)\",\"MadMix(12b)\",\"DarkPlanet(12b)\",\"UnslopNemoV4.1(12b)\",\"Violet Lotus(12b)\",\"Abomination Science(12b)\",\"DarkAtom(12b)\",\"CaptainErisViolet(12b)\",\"Ink(12b)\",\"Wayfarer(12b)\",\"TieFighter(13b)\",\"Psyfighter(13b)\",\"Psyfighter2(13b)\",\"PsyMedRP(13b)\",\"EstopianMaid(13b)\",\"Noromaid0.1(13b)\",\"EVA-Qwen2.5 (14b)\",\"Eidolon(14b)\",\"EVA-Tissint(14b)\",\"Sugarquill(14b)\",\"Freya(14b)\",\"Kunou(14b)\",\"Sailor2(14b)\",\"Sailor2-Chat(14b)\",\"Deepseek-Kunou(14b)\",\"L3.1 OpenCrystal(15b)\",\"Cydonia(22b)\",\"Magnum(22b)\",\"Sorcerer(22b)\"]{allow-input: true}\n",
        "\n",
        "Context = \"16384\" # @param [\"65536\",\"32768\",\"24576\",\"16384\",\"12288\",\"8192\",\"6144\",\"4096\", \"3072\", \"2048\"]\n",
        "\n",
        "Instruct_Preset = \"chatml\" # @param [\"alpaca\", \"vicuna\", \"llama-3\", \"command-r\", \"chatml\", \"mistral\", \"gemma2\", \"metharme\"]\n",
        "# @markdown model will works best if using the same instruct it was trained for. certain model (e.g. all llama3 models) will become idiot if the instruct is mismatched. Please refer to the table above.\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Advanced setting\n",
        "Layers = 99 # @param {type:\"number\"}\n",
        "# @markdown Reduce layer will make generation slower, but it also save VRAM. So you can have more context size.\n",
        "KvCache = \"0\" # @param [\"0\", \"1\", \"2\"]\n",
        "# @markdown Kvcache will reduce a bit of respose quality to save VRAM. But it will reprocess prompt every time new message is sent.\n",
        "\n",
        "blayer = 99\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "force_redownload_kobold = False # @param {type:\"boolean\"}\n",
        "\n",
        "provider = \"Cloudflare\" # @param [\"Cloudflare\", \"Localtunnel\"]\n",
        "# @markdown ##### If stuck at \"Checking if the server is up...\" while using cloudflare, change provider to localtunnel may help\n",
        "# @markdown ##### Check force redownload if you try everything but it still not working.\n",
        "modellink = ''\n",
        "\n",
        "# declare context template\n",
        "\n",
        "premade_instruct = {\n",
        "    \"alpaca\": {\n",
        "        \"system_start\": \"\\n### Input: \",\n",
        "        \"system_end\": \"\",\n",
        "        \"user_start\": \"\\n### Instruction: \",\n",
        "        \"user_end\": \"\",\n",
        "        \"assistant_start\": \"\\n### Response: \",\n",
        "        \"assistant_end\": \"\",\n",
        "    },\n",
        "    \"vicuna\": {\n",
        "        \"system_start\": \"\\nSYSTEM: \",\n",
        "        \"system_end\": \"\",\n",
        "        \"user_start\": \"\\nUSER: \",\n",
        "        \"user_end\": \"\",\n",
        "        \"assistant_start\": \"\\nASSISTANT: \",\n",
        "        \"assistant_end\": \"\",\n",
        "    },\n",
        "    \"llama-3\": {\n",
        "        \"system_start\": \"<|start_header_id|>system<|end_header_id|>\\n\\n\",\n",
        "        \"system_end\": \"<|eot_id|>\",\n",
        "        \"user_start\": \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "        \"user_end\": \"<|eot_id|>\",\n",
        "        \"assistant_start\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "        \"assistant_end\": \"<|eot_id|>\",\n",
        "    },\n",
        "    \"chatml\": {\n",
        "        \"system_start\": \"<|im_start|>system\",\n",
        "        \"system_end\": \"<|im_end|>\",\n",
        "        \"user_start\": \"<|im_start|>user\",\n",
        "        \"user_end\": \"<|im_end|>\",\n",
        "        \"assistant_start\": \"<|im_start|>assistant\",\n",
        "        \"assistant_end\": \"<|im_end|>\",\n",
        "    },\n",
        "    \"command-r\": {\n",
        "        \"system_start\": \"<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\",\n",
        "        \"system_end\": \"<|END_OF_TURN_TOKEN|>\",\n",
        "        \"user_start\": \"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>\",\n",
        "        \"user_end\": \"<|END_OF_TURN_TOKEN|>\",\n",
        "        \"assistant_start\": \"<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\",\n",
        "        \"assistant_end\": \"<|END_OF_TURN_TOKEN|>\",\n",
        "    },\n",
        "    \"mistral\":  {\n",
        "      \"system_start\": \"\",\n",
        "      \"system_end\": \"\",\n",
        "      \"user_start\": \"[INST] \",\n",
        "      \"user_end\": \"\",\n",
        "      \"assistant_start\": \" [/INST]\",\n",
        "      \"assistant_end\": \"</s> \"\n",
        "    },\n",
        "    \"gemma2\":{\n",
        "      \"system_start\": \"<start_of_turn>system\\n\",\n",
        "      \"system_end\": \"<end_of_turn>\\n\",\n",
        "      \"user_start\": \"<start_of_turn>user\\n\",\n",
        "      \"user_end\": \"<end_of_turn>\\n\",\n",
        "      \"assistant_start\": \"<start_of_turn>model\\n\",\n",
        "      \"assistant_end\": \"<end_of_turn>\\n\"\n",
        "    },\n",
        "    \"metharme\": {\n",
        "      \"system_start\": \"<|system|>\",\n",
        "      \"system_end\": \"\",\n",
        "      \"user_start\": \"<|user|>\",\n",
        "      \"user_end\": \"\",\n",
        "      \"assistant_start\": \"<|model>\",\n",
        "      \"assistant_end\": \"\"\n",
        "    }\n",
        "}\n",
        "\n",
        "if \"https\" in Model:\n",
        "  modellink = Model\n",
        "else:\n",
        "  match Model:\n",
        "    case \"Kunoichi(7b)\" :\n",
        "        modellink = \"https://huggingface.co/Lewdiculous/Kunoichi-DPO-v2-7B-GGUF-Imatrix/resolve/main/Kunoichi-DPO-v2-7B-Q8_0-imatrix.gguf?download=true\"\n",
        "    case \"WizardIceLemonTeaRP(7b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/WizardIceLemonTeaRP-32k-GGUF/resolve/main/WizardIceLemonTeaRP-32k.Q8_0.gguf?download=true\"\n",
        "    case \"WizardLaker(7b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/WizardLaker-7B-GGUF/resolve/main/WizardLaker-7B.Q8_0.gguf?download=true\"\n",
        "    case \"StunnaMaid(7b)\" :\n",
        "        modellink = \"https://huggingface.co/Lewdiculous/Nyanade_Stunna-Maid-7B-v0.2-GGUF-IQ-Imatrix/resolve/main/Nyanade_Stunna-Maid-7B-v0.2-Q8_0-imat.gguf?download=true\"\n",
        "    case \"LemonKunoichiWizard(7b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/LemonKunoichiWizardV3-GGUF/resolve/main/LemonKunoichiWizardV3.Q8_0.gguf?download=true\"\n",
        "    case \"Llama-3-Halu-Blackroot(8k)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/Halu-8B-Llama3-Blackroot-GGUF/resolve/main/Halu-8B-Llama3-Blackroot.Q8_0.gguf?download=true\"\n",
        "    case \"Llama-3-Lumimaid(8b)\" :\n",
        "        modellink = \"https://huggingface.co/Lewdiculous/Llama-3-Lumimaid-8B-v0.1-OAS-GGUF-IQ-Imatrix/resolve/main/Llama-3-Lumimaid-8B-v0.1-OAS-Q8_0-imat.gguf?download=true\"\n",
        "    case \"Llama-3-Daybreak-Lumimaid(8b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/llama3-daybreak-lumimaid0.1-8b-hf-GGUF/resolve/main/llama3-daybreak-lumimaid0.1-8b-hf.Q8_0.gguf?download=true\"\n",
        "    case \"Llama-3-Stheno(8b)\" :\n",
        "        modellink = \"https://huggingface.co/QuantFactory/Llama-3.1-8B-Stheno-v3.4-GGUF/resolve/main/Llama-3.1-8B-Stheno-v3.4.Q8_0.gguf?download=true\"\n",
        "    case \"Llama-3-Stheno-ULTRA(8b)\" :\n",
        "        modellink = \"https://huggingface.co/DavidAU/L3-8B-Stheno-v3.3-32K-Ultra-NEO-V1-IMATRIX-GGUF/resolve/main/L3-8B-Stheno-v3.3-32K-NEO-V1-D_AU-Q8_0-imat13.gguf?download=true\"\n",
        "    case \"Llama-3-SthenoMaidBlackroot(8b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/L3-SthenoMaidBlackroot-8B-V1-GGUF/resolve/main/L3-SthenoMaidBlackroot-8B-V1.Q8_0.gguf?download=true\"\n",
        "    case \"Llama-3-Umbral-Mind(8b)\" :\n",
        "        modellink = \"https://huggingface.co/QuantFactory/L3-Umbral-Mind-RP-v3.0-8B-GGUF/resolve/main/L3-Umbral-Mind-RP-v3.0-8B.Q8_0.gguf?download=true\"\n",
        "    case \"Llama-3-Hathor-Stable(8b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/Hathor_Stable-v0.2-L3-8B-GGUF/resolve/main/Hathor_Stable-v0.2-L3-8B.Q8_0.gguf?download=true\"\n",
        "    case \"Llama-3-Chara-Alpha(8b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/L3-8B-Chara-v1-Alpha-GGUF/resolve/main/L3-8B-Chara-v1-Alpha.Q8_0.gguf?download=true\"\n",
        "    case \"Llama-3-Hathor-Sofit(8b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/Hathor_Sofit-L3-8B-v1-GGUF/resolve/main/Hathor_Sofit-L3-8B-v1.Q8_0.gguf?download=true\"\n",
        "    case \"Llama-3-Lunaris(8b)\" :\n",
        "        modellink = \"https://huggingface.co/bartowski/L3-8B-Lunaris-v1-GGUF/resolve/main/L3-8B-Lunaris-v1-Q8_0_L.gguf?download=true\"\n",
        "    case \"Llama-3.1-Dark-Planet-8-Orbs(8b)\" :\n",
        "        modellink = \"https://huggingface.co/DavidAU/L3-Dark-Planet-8B-V2-Eight-Orbs-Of-Power-GGUF/resolve/main/L3-Dark-Planet-8B-V2-EOOP-D_AU-Q8_0.gguf?download=true\"\n",
        "    case \"Llama-3.1-DarkIdol(8b)\" :\n",
        "        modellink = \"https://huggingface.co/QuantFactory/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored-GGUF/resolve/main/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0.gguf?download=true\"\n",
        "    case \"Yodayo-Nephra(8b)\" :\n",
        "        modellink = \"https://huggingface.co/Marcus-Arcadius/nephra_v1.0-Q8_0-GGUF/resolve/main/nephra_v1.0-q8_0.gguf?download=true\"\n",
        "    case \"Gemma2-SPPO(9b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/Gemma-2-9B-It-SPPO-Iter3-i1-GGUF/resolve/main/Gemma-2-9B-It-SPPO-Iter3.i1-Q6_K.gguf?download=true\"\n",
        "    case \"Tarnished(9b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/tarnished-9b-GGUF/resolve/main/tarnished-9b.Q6_K.gguf?download=true\"\n",
        "    case \"Gemma2-Daybreak(9b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/gemma2-9B-daybreak-v0.5-i1-GGUF/resolve/main/gemma2-9B-daybreak-v0.5.i1-Q6_K.gguf?download=true\"\n",
        "    case \"Gemma2-Sunfall(9b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/gemma2-9B-sunfall-v0.5.2-i1-GGUF/resolve/main/gemma2-9B-sunfall-v0.5.2.i1-Q6_K.gguf?download=true\"\n",
        "    case \"Gemma2-Ataraxy(9b)\" :\n",
        "        modellink = \"https://huggingface.co/bartowski/Gemma-2-Ataraxy-9B-GGUF/resolve/main/Gemma-2-Ataraxy-9B-Q5_K_L.gguf?download=true\"\n",
        "    case \"Fimbulvetr2(11b)\" :\n",
        "        modellink = \"https://huggingface.co/Lewdiculous/Fimbulvetr-11B-v2-GGUF-IQ-Imatrix/resolve/main/Fimbulvetr-11B-v2-Q5_K_M-imat.gguf?download=true\"\n",
        "    case \"Fimbulvetr-Kuro-Lotus(11b)\" :\n",
        "        modellink = \"https://huggingface.co/saishf/Fimbulvetr-Kuro-Lotus-10.7B-GGUF/resolve/main/Fimbulvetr-Kuro-Lotus-10.7B-Q6_K.gguf?download=true\"\n",
        "    case \"Kaiju(11b)\" :\n",
        "        modellink = \"https://huggingface.co/Himitsui/Kaiju-11B-GGUF/resolve/main/Kaiju-11B.q5_K_M.gguf?download=true\"\n",
        "    case \"Fimbulvetr-Holodeck-Erebus-Westlake(11b)\" :\n",
        "        modellink = \"https://huggingface.co/PJMixers/Fimbulvetr-Holodeck-Erebus-Westlake-10.7B-GGUF/resolve/main/Fimbulvetr-Holodeck-Erebus-Westlake-10.7B-q4_K_S.gguf\"\n",
        "    case \"MoistralV3(11b)\" :\n",
        "        modellink = \"https://huggingface.co/TheDrummer/Moistral-11B-v3-GGUF/resolve/main/Moistral-11B-v3-Q5_K_M.gguf?download=true\"\n",
        "    case \"Lumimaid(12b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/Lumimaid-v0.2-12B-i1-GGUF/resolve/main/Lumimaid-v0.2-12B.i1-Q6_K.gguf?download=true\"\n",
        "    case \"Mini-Magnum(12b)\" :\n",
        "        modellink = \"https://huggingface.co/InferenceIllusionist/mini-magnum-12b-v1.1-iMat-GGUF/resolve/main/mini-magnum-12b-v1.1-iMat-Q6_K.gguf?download=true\"\n",
        "    case \"Nemomix(12b)\" :\n",
        "        modellink = \"https://huggingface.co/bartowski/NemoMix-Unleashed-12B-GGUF/resolve/main/NemoMix-Unleashed-12B-Q6_K.gguf?download=true\"\n",
        "    case \"Celeste(12b)\" :\n",
        "        modellink = \"https://huggingface.co/QuantFactory/Celeste-12B-V1.6-GGUF/resolve/main/Celeste-12B-V1.6.Q6_K.gguf?download=true\"\n",
        "    case \"Lyra(12b)\" :\n",
        "        modellink = \"https://huggingface.co/Lewdiculous/MN-12B-Lyra-v4-GGUF-IQ-Imatrix/resolve/main/MN-12B-Lyra-v4-Q6_K-imat.gguf?download=true\"\n",
        "    case \"Guns-and-roses(12b)\" :\n",
        "        modellink = \"https://huggingface.co/Reiterate3680/guns-and-roses-r1-GGUF/resolve/main/guns-and-roses-r1-Q6_K_L-imat.gguf?download=true\"\n",
        "    case \"Magnum(12b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/magnum-v4-12b-GGUF/resolve/main/magnum-v4-12b.Q6_K.gguf?download=true\"\n",
        "    case \"Starcannon(12b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/MN-12B-Starcannon-v3-i1-GGUF/resolve/main/MN-12B-Starcannon-v3.i1-Q6_K.gguf?download=true\"\n",
        "    case \"Rocinante(12b)\" :\n",
        "        modellink = \"https://huggingface.co/TheDrummer/UnslopNemo-12B-v3-GGUF/resolve/main/Rocinante-12B-v2g-Q6_K.gguf?download=true\"\n",
        "    case \"Chronos Gold(12b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/Chronos-Gold-12B-1.0-i1-GGUF/resolve/main/Chronos-Gold-12B-1.0.i1-Q6_K.gguf?download=true\"\n",
        "    case \"Mag-Mell(12b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/MN-12B-Mag-Mell-R1-GGUF/resolve/main/MN-12B-Mag-Mell-R1.Q6_K.gguf?download=true\"\n",
        "    case \"Violet Twilight(12b)\" :\n",
        "        modellink = \"https://huggingface.co/Epiculous/Violet_Twilight-v0.2-GGUF/resolve/main/Violet_Twilight-v0.2.Q6_K.gguf?download=true\"\n",
        "    case \"Halide(12b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/MN-Halide-12b-v1.0-i1-GGUF/resolve/main/MN-Halide-12b-v1.0.i1-Q6_K.gguf?download=true\"\n",
        "    case \"Stellar Odyssy(12b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/Stellar-Odyssey-12b-v0.0-i1-GGUF/resolve/main/Stellar-Odyssey-12b-v0.0.i1-Q6_K.gguf?download=true\"\n",
        "    case \"MadMix(12b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/MadMix-Unleashed-12B-i1-GGUF/resolve/main/MadMix-Unleashed-12B.i1-Q6_K.gguf?download=true\"\n",
        "    case \"DarkPlanet(12b)\" :\n",
        "        modellink = \"https://huggingface.co/DavidAU/MN-Dark-Planet-TITAN-12B-GGUF/resolve/main/MN-Dark-Planet-TITAN-12B-D_AU-Q6_k.gguf?download=true\"\n",
        "    case \"UnslopNemoV4.1(12b)\" :\n",
        "        modellink = \"https://huggingface.co/TheDrummer/UnslopNemo-12B-v4.1-GGUF/resolve/main/Rocinante-12B-v2j-Q6_K.gguf?download=true\"\n",
        "    case \"UnslopNemoV4.1(12b)\" :\n",
        "        modellink = \"https://huggingface.co/TheDrummer/UnslopNemo-12B-v4.1-GGUF/resolve/main/Rocinante-12B-v2j-Q6_K.gguf?download=true\"\n",
        "    case \"Violet Lotus(12b)\" :\n",
        "        modellink = \"https://huggingface.co/QuantFactory/MN-Violet-Lotus-12B-GGUF/resolve/main/MN-Violet-Lotus-12B.Q6_K.gguf?download=true\"\n",
        "    case \"Abomination Science(12b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/AbominationScience-12B-v4-i1-GGUF/resolve/main/AbominationScience-12B-v4.i1-Q6_K.gguf?download=true\"\n",
        "    case \"DarkAtom(12b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/DarkAtom-12B-v3-i1-GGUF/resolve/main/DarkAtom-12B-v3.i1-Q6_K.gguf?download=true\"\n",
        "    case \"CaptainErisViolet(12b)\" :\n",
        "        modellink = \"https://huggingface.co/QuantFactory/Captain-Eris_Violet-V0.420-12B-GGUF/resolve/main/Captain-Eris_Violet-V0.420-12B.Q5_K_M.gguf?download=true\"\n",
        "    case \"Ink(12b)\" :\n",
        "        modellink = \"https://huggingface.co/allura-org/MN-12b-RP-Ink-GGUF/resolve/main/MN-12b-RP-Ink-Q5_K_M.gguf?download=true\"\n",
        "    case \"Wayfarer(12b)\" :\n",
        "        modellink = \"https://huggingface.co/LatitudeGames/Wayfarer-12B-GGUF/resolve/main/Wayfarer-12B-Q5_K_M.gguf?download=true\"\n",
        "    case \"TieFighter(13b)\" :\n",
        "        modellink = \"https://huggingface.co/KoboldAI/LLaMA2-13B-Tiefighter-GGUF/resolve/main/LLaMA2-13B-Tiefighter.Q5_K_M.gguf\"\n",
        "    case \"Psyfighter(13b)\" :\n",
        "        modellink = \"https://huggingface.co/TheBloke/Psyfighter-13B-GGUF/resolve/main/psyfighter-13b.Q5_K_M.gguf\"\n",
        "    case \"Psyfighter2(13b)\" :\n",
        "        modellink = \"https://huggingface.co/KoboldAI/LLaMA2-13B-Psyfighter2-GGUF/resolve/main/LLaMA2-13B-Psyfighter2.Q4_K_M.gguf\"\n",
        "    case \"PsyMedRP(13b)\" :\n",
        "        modellink = \"https://huggingface.co/Undi95/PsyMedRP-v1-13B-GGUF/resolve/main/PsyMedRP-v1-13B.q5_k_m.gguf\"\n",
        "    case \"EstopianMaid(13b)\" :\n",
        "        modellink = \"https://huggingface.co/KatyTheCutie/EstopianMaid-13B-GGUF/resolve/main/EstopianMaid-13B-Q4_K_S.gguf\"\n",
        "    case \"Noromaid0.1(13b)\" :\n",
        "        modellink = \"https://huggingface.co/NeverSleep/Noromaid-13b-v0.1.1-GGUF/resolve/main/Noromaid-13b-v0.1.1.q4_k_s.gguf\"\n",
        "    case \"L3.1 OpenCrystal(12b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/OpenCrystal-12B-L3-i1-GGUF/resolve/main/OpenCrystal-12B-L3.i1-Q6_K.gguf?download=true\"\n",
        "    case \"EVA-Qwen2.5 (14b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/EVA-Qwen2.5-14B-v0.2-i1-GGUF/resolve/main/EVA-Qwen2.5-14B-v0.2.i1-Q5_K_M.gguf?download=true\"\n",
        "    case \"Eidolon(14b)\" :\n",
        "        modellink = \"https://huggingface.co/Lambent/Eidolon-v2.1-14B-Q4_K_M-GGUF/resolve/main/eidolon-v2.1-14b-q4_k_m.gguf?download=true\"\n",
        "    case \"EVA-Tissint(14b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/EVA-Tissint-v1.2-14B-i1-GGUF/resolve/main/EVA-Tissint-v1.2-14B.i1-Q5_K_M.gguf?download=true\"\n",
        "    case \"Sugarquill(14b)\" :\n",
        "        modellink = \"https://huggingface.co/Triangle104/TQ2.5-14B-Sugarquill-v1-Q5_K_M-GGUF/resolve/main/tq2.5-14b-sugarquill-v1-q5_k_m.gguf?download=true\"\n",
        "    case \"Freya(14b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/14B-Qwen2.5-Freya-x1-i1-GGUF/resolve/main/14B-Qwen2.5-Freya-x1.i1-Q5_K_M.gguf?download=true\"\n",
        "    case \"Kunou(14b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/14B-Qwen2.5-Kunou-v1-GGUF/resolve/main/14B-Qwen2.5-Kunou-v1.Q5_K_M.gguf?download=true\"\n",
        "    case \"Sailor2(14b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/Sailor2-14B-GGUF/resolve/main/Sailor2-14B.Q4_K_S.gguf?download=true\"\n",
        "    case \"Sailor2-Chat(14b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/Sailor2-14B-Chat-GGUF/resolve/main/Sailor2-14B-Chat.Q4_K_S.gguf?download=true\"\n",
        "    case \"Deepseek-Kunou(14b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/Deepseeker-Kunou-Qwen2.5-14b-i1-GGUF/resolve/main/Deepseeker-Kunou-Qwen2.5-14b.i1-Q5_K_M.gguf?download=true\"\n",
        "    case \"L3.1 OpenCrystal(15b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/OpenCrystal-15B-L3-v2-i1-GGUF/resolve/main/OpenCrystal-15B-L3-v2.i1-Q6_K.gguf?download=true\"\n",
        "    case \"Cydonia(22b)\" :\n",
        "        modellink = \"https://huggingface.co/MarsupialAI/Cydonia-22B-v1_iMat_GGUF/resolve/main/Cydonia-22B-v1_iQ4xs.gguf?download=true\"\n",
        "    case \"Magnum(22b)\" :\n",
        "        modellink = \"https://huggingface.co/mradermacher/magnum-v4-22b-i1-GGUF/resolve/main/magnum-v4-22b.i1-IQ4_XS.gguf?download=true\"\n",
        "    case \"Sorcerer(22b)\" :\n",
        "        modellink = \"https://huggingface.co/Quant-Cartel/SorcererLM-22B-iMat-GGUF/resolve/main/SorcererLM-22B-iMat-IQ4_XS.gguf?download=true\"\n",
        "\n",
        "!echo Downloading KoboldCpp, please wait...\n",
        "!wget -O dlfile.tmp https://kcpplinux.concedo.workers.dev && mv dlfile.tmp koboldcpp_linux\n",
        "!test -f koboldcpp_linux && echo Download Successful || echo Download Failed\n",
        "!chmod +x ./koboldcpp_linux\n",
        "!apt update\n",
        "!apt install aria2 -y\n",
        "\n",
        "modelname = modellink.split('/')[-1].split('.')[0]\n",
        "matched = re.search(r'/([^/]+)-GGUF/', modellink)\n",
        "remodelname = (\"koboldcpp/model\")\n",
        "if matched:\n",
        "    remodelname = f\"koboldcpp/{matched.group(1)}\"\n",
        "isModelExist = os.path.isfile('model.txt')\n",
        "oldmodel = \"\"\n",
        "if(isModelExist):\n",
        "  f = open(\"model.txt\", \"r\")\n",
        "  oldmodel = f.read()\n",
        "  print(f'oldmodel = {oldmodel}')\n",
        "\n",
        "if(oldmodel != modellink or isModelExist == False):\n",
        "  print('Download: '+ modellink)\n",
        "  !aria2c -x 16 -s 16 -k 1M -o model.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $modellink\n",
        "  # !aria2c -x 16 -s 16 -k 1M --allow-overwrite=\"true\" --summary-interval=5 $modellink -o model.gguf 2>&1 | grep -Ev 'Redirecting'\n",
        "  with open('model.txt', 'w') as f:\n",
        "      f.write(modellink)\n",
        "\n",
        "#create instruct file\n",
        "with open(\"instruct.json\", \"w\") as f:\n",
        "    f.write(json.dumps(premade_instruct[Instruct_Preset], separators=(\",\", \":\")))\n",
        "\n",
        "if provider == \"Localtunnel\":\n",
        "  !npm install -g localtunnel\n",
        "  print('\\n')\n",
        "  !echo > nohup.out\n",
        "  !nohup lt --port 5001 &\n",
        "  print(\"Checking if the server is up...\\n\")\n",
        "  while True:\n",
        "      time.sleep(1)\n",
        "      with open('nohup.out', 'r') as f:\n",
        "        if 'your url is' in f.read():\n",
        "            print('=============================================================================')\n",
        "            print('please verify ip of colab in the loca.lt link before using it as kobold url')\n",
        "            print('colab ip: ', end='')\n",
        "            !curl ipecho.net/plain\n",
        "            !cat nohup.out\n",
        "            print('===========================================================================')\n",
        "            print(\"--------------------------\\nServer up!\")\n",
        "            break\n",
        "  print(\"--------------------------\\n\")\n",
        "\n",
        "tunneling = \"\"\n",
        "if provider == \"Cloudflare\":\n",
        "  tunneling = \"--remotetunnel\"\n",
        "!./koboldcpp_linux model.gguf --usecublas 0 mmq --multiuser --gpulayers $Layers --contextsize $Context --flashattention --hordemodelname $remodelname --quiet --remotetunnel --chatcompletionsadapter instruct.json\n",
        "\n",
        "# !python koboldcpp.py /content/model/model.gguf --usecublas normal 0 mmq --gpulayers $Layers --context $Context $tunneling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If you encounter bugs or want to request specific model. Ping @Hibikiass on janitor discord\n"
      ],
      "metadata": {
        "id": "hpSHYEYCea_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Spreadsheet to generate code for colab (not for user. this is just for me to quickly generating code for model to add in this colab)\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1jdiZ3uW9MJSjrUwBABjUA9e95q91rIaSpJ-xwClZ0qM"
      ],
      "metadata": {
        "id": "rciwENgWzv-v"
      }
    }
  ]
}